{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP DRAGON FLY\\AppData\\Local\\Temp\\ipykernel_22468\\255736262.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)  # Convert to numeric, replace errors with NaN, and then fill NaNs with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.5758211612701416\n",
      "Epoch 10, Loss: 2.38887095451355\n",
      "Epoch 20, Loss: 2.16355037689209\n",
      "Epoch 30, Loss: 1.9588457345962524\n",
      "Epoch 40, Loss: 1.780250072479248\n",
      "Model Accuracy: 34.59%\n",
      "\n",
      "Training Loss and Accuracy Table:\n",
      "    Epoch      Loss   Accuracy  Learning Rate  Training Rate (Epochs)\n",
      "0       0  2.575821       None          0.001                      50\n",
      "1       1  2.557789       None          0.001                      50\n",
      "2       2  2.540076       None          0.001                      50\n",
      "3       3  2.522427       None          0.001                      50\n",
      "4       4  2.504715       None          0.001                      50\n",
      "5       5  2.486748       None          0.001                      50\n",
      "6       6  2.468320       None          0.001                      50\n",
      "7       7  2.449384       None          0.001                      50\n",
      "8       8  2.429842       None          0.001                      50\n",
      "9       9  2.409660       None          0.001                      50\n",
      "10     10  2.388871       None          0.001                      50\n",
      "11     11  2.367510       None          0.001                      50\n",
      "12     12  2.345612       None          0.001                      50\n",
      "13     13  2.323257       None          0.001                      50\n",
      "14     14  2.300550       None          0.001                      50\n",
      "15     15  2.277607       None          0.001                      50\n",
      "16     16  2.254571       None          0.001                      50\n",
      "17     17  2.231560       None          0.001                      50\n",
      "18     18  2.208658       None          0.001                      50\n",
      "19     19  2.185961       None          0.001                      50\n",
      "20     20  2.163550       None          0.001                      50\n",
      "21     21  2.141480       None          0.001                      50\n",
      "22     22  2.119778       None          0.001                      50\n",
      "23     23  2.098468       None          0.001                      50\n",
      "24     24  2.077542       None          0.001                      50\n",
      "25     25  2.057000       None          0.001                      50\n",
      "26     26  2.036811       None          0.001                      50\n",
      "27     27  2.016930       None          0.001                      50\n",
      "28     28  1.997347       None          0.001                      50\n",
      "29     29  1.978002       None          0.001                      50\n",
      "30     30  1.958846       None          0.001                      50\n",
      "31     31  1.939888       None          0.001                      50\n",
      "32     32  1.921140       None          0.001                      50\n",
      "33     33  1.902627       None          0.001                      50\n",
      "34     34  1.884369       None          0.001                      50\n",
      "35     35  1.866383       None          0.001                      50\n",
      "36     36  1.848650       None          0.001                      50\n",
      "37     37  1.831187       None          0.001                      50\n",
      "38     38  1.813978       None          0.001                      50\n",
      "39     39  1.797007       None          0.001                      50\n",
      "40     40  1.780250       None          0.001                      50\n",
      "41     41  1.763685       None          0.001                      50\n",
      "42     42  1.747309       None          0.001                      50\n",
      "43     43  1.731103       None          0.001                      50\n",
      "44     44  1.715059       None          0.001                      50\n",
      "45     45  1.699188       None          0.001                      50\n",
      "46     46  1.683502       None          0.001                      50\n",
      "47     47  1.668007       None          0.001                      50\n",
      "48     48  1.652711       None          0.001                      50\n",
      "49     49  1.637612  34.589041          0.001                      50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# File path to your dataset\n",
    "file_path = r\"fulldataset.xlsx\"\n",
    "# Load the dataset\n",
    "data = pd.read_excel(file_path, engine='openpyxl', dtype=str)\n",
    "\n",
    "## **Data Preparation**\n",
    "\n",
    "# Convert all column names to lowercase and remove spaces\n",
    "data.columns = data.columns.str.strip().str.lower()\n",
    "\n",
    "# Define the selected features based on relevance\n",
    "selected_features = [\n",
    "    'temp', 'nasal', 'eye', 'ears', 'cough', 'weight', 'total_steps', 'total_mi',\n",
    "    'lying_bouts', 'lying_daily', 'milk_intake', 'milk_percent', 'starter_intake',\n",
    "    'speed', 'speed_percent', 'brix', 'fecal', 'ultrasound'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'brd_total'\n",
    "\n",
    "# Check for missing columns and remove them\n",
    "missing_cols = [col for col in selected_features if col not in data.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns in the dataset: {missing_cols}\")\n",
    "    selected_features = [col for col in selected_features if col in data.columns]\n",
    "\n",
    "# Fill missing values with 0 or a more appropriate value\n",
    "data[selected_features] = data[selected_features].fillna(0)  # Use 0 or another suitable value\n",
    "\n",
    "# Collect features and target variable\n",
    "X = data[selected_features]\n",
    "y = data[target]\n",
    "\n",
    "# Convert non-numeric columns to numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':  # Check if the column is of object type\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)  # Convert to numeric, replace errors with NaN, and then fill NaNs with 0\n",
    "\n",
    "# Fill any remaining NaN values in X with 0\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert target values to numeric and handle missing values\n",
    "y_train = pd.to_numeric(y_train, errors='coerce').fillna(0)\n",
    "y_test = pd.to_numeric(y_test, errors='coerce').fillna(0)\n",
    "\n",
    "# Ensure target values start from 0 and are consecutive\n",
    "y_train_unique = y_train.unique()\n",
    "target_mapping = {val: i for i, val in enumerate(y_train_unique)}\n",
    "y_train = y_train.map(target_mapping).astype(int)  # Ensure integer type\n",
    "y_test = y_test.map(target_mapping).astype(int)  # Ensure integer type\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # Use .values to access the NumPy array\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)  # Use .values to access the NumPy array\n",
    "\n",
    "## **Define Deep Q Knowledge Distillation Model**\n",
    "class DQKD(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQKD, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Model Initialization\n",
    "model = DQKD(input_dim=X_train_tensor.shape[1], output_dim=len(y_train_unique))\n",
    "\n",
    "## **Loss function and Optimizer**\n",
    "learning_rate = 0.001  # Define learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## **Training the model**\n",
    "def train_model(model, X_train, y_train, epochs=50):\n",
    "    loss_values = []  # List to capture loss values at each epoch\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_values.append(loss.item())  # Append the loss value to the list\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    \n",
    "    return loss_values  # Return the list of loss values\n",
    "\n",
    "loss_values = train_model(model, X_train_tensor, y_train_tensor)\n",
    "\n",
    "## **Evaluating Model Performance**\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_labels = torch.argmax(y_pred, axis=1)\n",
    "    accuracy = (y_pred_labels == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    print(f'Model Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# **Tabulate Results**\n",
    "# Create a DataFrame to display the loss values per epoch and the final accuracy\n",
    "results = {\n",
    "    'Epoch': list(range(50)),\n",
    "    'Loss': loss_values + [None] * (50 - len(loss_values))  # Pad loss values to 50 epochs if less\n",
    "}\n",
    "\n",
    "# Adding accuracy at the end of the table\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['Accuracy'] = [None] * len(results_df)\n",
    "results_df.loc[49, 'Accuracy'] = accuracy * 100  # Insert accuracy in the last row\n",
    "\n",
    "# Adding learning rate and training rate to the table\n",
    "results_df['Learning Rate'] = learning_rate\n",
    "results_df['Training Rate (Epochs)'] = 50  # Training rate is the number of epochs\n",
    "\n",
    "# Display the tabulated results\n",
    "print(\"\\nTraining Loss and Accuracy Table:\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
