{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Hyperparameters\n",
    "state_size = None\n",
    "action_size = 2  # Binary classification: BRD or no BRD\n",
    "batch_size = 64\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "memory_size = 10000\n",
    "train_start = 1000\n",
    "target_update_interval = 10  # Update target model every 10 episodes\n",
    "training_rate = 0.01  # Add a training rate hyperparameter\n",
    "\n",
    "def load_data():\n",
    "    global state_size\n",
    "    df = pd.read_excel('fulldataset.xlsx')\n",
    "\n",
    "    # --- Data Preprocessing ---\n",
    "    # Fill NaN values with the mode for object columns and median for numerical\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # Convert date columns to datetime and extract features\n",
    "    date_columns = ['Date', 'Enrolldate', 'BIRTHDATE']\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        df[f'{col}_year'] = df[col].dt.year\n",
    "        df[f'{col}_month'] = df[col].dt.month\n",
    "        df[f'{col}_day'] = df[col].dt.day\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        df[col] = pd.factorize(df[col])[0]\n",
    "\n",
    "    # Define target and features\n",
    "    target_column = 'BRD_Total'\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Store state size\n",
    "    state_size = X_scaled.shape[1]\n",
    "    print(f\"Shape of X: {X_scaled.shape}\")\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "class DQNModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.dense1 = layers.Dense(128, activation='relu', kernel_initializer='he_uniform')  # Increased units, better initialization\n",
    "        self.dense2 = layers.Dense(128, activation='relu', kernel_initializer='he_uniform')  # Increased units, better initialization\n",
    "        self.output_layer = layers.Dense(action_size, activation='linear', kernel_initializer='he_uniform')  # Linear output\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, maxlen=memory_size):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)  # More efficient sampling\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        return zip(*samples)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def train_dqn(model, target_model, buffer, optimizer, X_train, y_train):  # Pass y_train as well\n",
    "    if buffer.size() < train_start:\n",
    "        return\n",
    "\n",
    "    # Sample a batch of experiences from the replay buffer\n",
    "    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "    states = np.array(states).reshape(batch_size, -1)  # Reshape states\n",
    "    next_states = np.array(next_states).reshape(batch_size, -1)  # Reshape next states\n",
    "    rewards = np.array(rewards, dtype=np.float32).reshape(-1, 1)\n",
    "    dones = np.array(dones, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    # Convert actions to numpy array\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute Q-values for the current states\n",
    "        q_values = model(states)  # Shape: (batch_size, action_size)\n",
    "\n",
    "        # Compute Q-values for the actions taken\n",
    "        action_masks = tf.one_hot(actions, action_size)  # Shape: (batch_size, action_size)\n",
    "        q_values_for_actions = tf.reduce_sum(q_values * action_masks, axis=1, keepdims=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = target_model(next_states)  # Shape: (batch_size, action_size)\n",
    "        max_next_q_values = tf.reduce_max(next_q_values, axis=1, keepdims=True)  # Shape: (batch_size, 1)\n",
    "        targets = rewards + gamma * max_next_q_values * (1 - dones)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = tf.reduce_mean(tf.square(targets - q_values_for_actions))\n",
    "\n",
    "    # Compute gradients and apply them\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "def select_action(model, state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randint(0, action_size - 1)  # Explore\n",
    "    else:\n",
    "        q_values = model(state.reshape(1, -1))  # Exploit: Choose action with highest Q-value\n",
    "        return np.argmax(q_values.numpy())\n",
    "\n",
    "def calculate_accuracy(model, X_test, y_test):\n",
    "    predictions = []\n",
    "    for state in X_test:\n",
    "        q_values = model(state.reshape(1, -1))\n",
    "        action = np.argmax(q_values.numpy())\n",
    "        predictions.append(action)\n",
    "    return np.mean(np.array(predictions) == np.array(y_test))\n",
    "\n",
    "class CattleEnvironment:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.index = 0\n",
    "        self.num_samples = len(X)  # Store the number of samples\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self.X[self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.index >= self.num_samples:\n",
    "            print(\"Episode is already done. Please reset.\")\n",
    "            return None, 0, True\n",
    "\n",
    "        state = self.X[self.index]\n",
    "        brd_label = self.y[self.index]\n",
    "        reward = 1 if action == brd_label else 0  # Positive reward for correct action\n",
    "        self.index += 1\n",
    "        done = self.index >= self.num_samples  # Check if the episode is done\n",
    "        next_state = self.X[self.index] if not done else state  # Return the same state if done\n",
    "        return next_state, reward, done\n",
    "\n",
    "def train():\n",
    "    global epsilon\n",
    "    X_train, y_train, X_test, y_test = load_data()  # Load training and testing data\n",
    "\n",
    "    if X_train is None or y_train is None or X_test is None or y_test is None:\n",
    "        print(\"Error loading data.\")\n",
    "        return\n",
    "\n",
    "    # Initialize environment, model, and target model\n",
    "    env = CattleEnvironment(X_train, y_train)  # Pass training data to the environment\n",
    "    model = DQNModel()\n",
    "    target_model = DQNModel()\n",
    "\n",
    "    # Build the model by passing a dummy state\n",
    "    dummy_state = np.zeros((1, state_size))\n",
    "    model(dummy_state)\n",
    "    target_model(dummy_state)\n",
    "\n",
    "    # Copy weights from model to target_model\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    buffer = ReplayBuffer()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    episodes = 200  # Reduced the number of episodes\n",
    "    best_accuracy = 0.0  # Keep track of the best accuracy\n",
    "\n",
    "    # Store results in a list\n",
    "    results = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(model, state, epsilon)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Only store and train if the returned values are valid\n",
    "            if next_state is not None:\n",
    "                buffer.store(state, action, reward, next_state, done)\n",
    "                total_reward += reward\n",
    "                train_dqn(model, target_model, buffer, optimizer, X_train, y_train)  # Pass the entire training set\n",
    "                state = next_state\n",
    "\n",
    "        # Update epsilon\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        # Update target model weights every target_update_interval episodes\n",
    "        if episode % target_update_interval == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = calculate_accuracy(model, X_test, y_test)\n",
    "        results.append([episode, accuracy * 100, epsilon, total_reward])  # Append results\n",
    "\n",
    "        # Save the model if it's the best so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            print(f\"New best accuracy: {best_accuracy:.4f}, saving model...\")\n",
    "\n",
    "    # Convert results to DataFrame and print\n",
    "    results_df = pd.DataFrame(results, columns=['Episode', 'Accuracy (%)', 'Epsilon', 'Total Reward'])\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best Accuracy on Test Set: {best_accuracy * 100:.2f}%\")\n",
    "    print(results_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Hyperparameters\n",
    "state_size = None\n",
    "action_size = 2  # Binary classification: BRD or no BRD\n",
    "batch_size = 64\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "memory_size = 10000\n",
    "train_start = 1000\n",
    "target_update_interval = 10  # Update target model every 10 episodes\n",
    "training_rate = 0.01  # Add a training rate hyperparameter\n",
    "\n",
    "def load_data():\n",
    "    global state_size\n",
    "    df = pd.read_csv('metadata.csv')\n",
    "\n",
    "    # --- Data Preprocessing ---\n",
    "    # 1. Handle 'status' for BRD label:\n",
    "    # Correctly fill NaN values in 'status' column\n",
    "    df['status'] = df['status'].fillna('not_removed')\n",
    "\n",
    "    # Map 'status' to numerical values\n",
    "    status_mapping = {'removed': 1, 'not_removed': 0}\n",
    "    df['status'] = df['status'].map(status_mapping)\n",
    "\n",
    "    # 2. Fill missing 'clade' based on the mode:\n",
    "    df['clade'] = df['clade'].fillna(df['clade'].mode()[0])\n",
    "\n",
    "    # 3. Convert 'collection_date' to datetime, handling missing values:\n",
    "    date_formats = [\"%b-%y\", \"%m-%Y\", \"%d-%m-%Y\", \"%m-%d-%y\", \"%b-%Y\"]\n",
    "\n",
    "    def convert_to_datetime(date_string):\n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                return pd.to_datetime(date_string, format=fmt)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        return pd.NaT\n",
    "\n",
    "    df['collection_date'] = df['collection_date'].astype(str)\n",
    "    df['collection_date'] = df['collection_date'].apply(convert_to_datetime)\n",
    "\n",
    "    # 4. Extract date features:\n",
    "    df['year'] = df['collection_date'].dt.year\n",
    "    df['month'] = df['collection_date'].dt.month\n",
    "    df['day'] = df['collection_date'].dt.day\n",
    "    df = df.drop('collection_date', axis=1)\n",
    "\n",
    "    # 5. Drop unnecessary identifier columns:\n",
    "    columns_to_drop = ['sample', 'strain', 'sample_title', 'organism', 'isolate', 'host']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    # --- Feature Encoding ---\n",
    "    # 6. Encode categorical variables:\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])  # Impute before encoding\n",
    "        df[col] = pd.factorize(df[col])[0]  # Use numeric codes\n",
    "\n",
    "    # Verify that 'status' column exists before using it as the target column\n",
    "    target_column = 'status'\n",
    "    if target_column not in df.columns:\n",
    "        print(f\"Error: Target column '{target_column}' not found in the DataFrame.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # 7. Separate features and target:\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "\n",
    "    # 8. Scale features:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 9. Store state size:\n",
    "    state_size = X_scaled.shape[1]\n",
    "    print(f\"Shape of X: {X_scaled.shape}\")\n",
    "\n",
    "    # 10. Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "class DQNModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.dense1 = layers.Dense(128, activation='relu', kernel_initializer='he_uniform')  # Increased units, better initialization\n",
    "        self.dense2 = layers.Dense(128, activation='relu', kernel_initializer='he_uniform')  # Increased units, better initialization\n",
    "        self.output_layer = layers.Dense(action_size, activation='linear', kernel_initializer='he_uniform')  # Linear output\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, maxlen=memory_size):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)  # More efficient sampling\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        return zip(*samples)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def train_dqn(model, target_model, buffer, optimizer, X_train):\n",
    "    if buffer.size() < train_start:\n",
    "        return\n",
    "\n",
    "    # Sample a batch of experiences from the replay buffer\n",
    "    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "    states = np.array(states).reshape(batch_size, -1)  # Reshape states\n",
    "    next_states = np.array(next_states).reshape(batch_size, -1)  # Reshape next states\n",
    "    rewards = np.array(rewards, dtype=np.float32).reshape(-1, 1)\n",
    "    dones = np.array(dones, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    # Convert actions to numpy array\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute Q-values for the current states\n",
    "        q_values = model(states)  # Shape: (batch_size, action_size)\n",
    "\n",
    "        # Compute Q-values for the actions taken\n",
    "        action_masks = tf.one_hot(actions, action_size)  # Shape: (batch_size, action_size)\n",
    "        q_values_for_actions = tf.reduce_sum(q_values * action_masks, axis=1, keepdims=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = target_model(next_states)  # Shape: (batch_size, action_size)\n",
    "        max_next_q_values = tf.reduce_max(next_q_values, axis=1, keepdims=True)  # Shape: (batch_size, 1)\n",
    "        targets = rewards + gamma * max_next_q_values * (1 - dones)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = tf.reduce_mean(tf.square(targets - q_values_for_actions))\n",
    "\n",
    "    # Compute gradients and apply them\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "def select_action(model, state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randint(0, action_size - 1)  # Explore\n",
    "    else:\n",
    "        q_values = model(state.reshape(1, -1))  # Exploit: Choose action with highest Q-value\n",
    "        return np.argmax(q_values.numpy())\n",
    "\n",
    "def calculate_accuracy(model, X_test, y_test):\n",
    "    predictions = []\n",
    "    for state in X_test:\n",
    "        q_values = model(state.reshape(1, -1))\n",
    "        action = np.argmax(q_values.numpy())\n",
    "        predictions.append(action)\n",
    "    return np.mean(np.array(predictions) == np.array(y_test))\n",
    "\n",
    "class CattleEnvironment:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.index = 0\n",
    "        self.num_samples = len(X)  # Store the number of samples\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self.X[self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.index >= self.num_samples:\n",
    "            print(\"Episode is already done. Please reset.\")\n",
    "            return None, 0, True\n",
    "\n",
    "        state = self.X[self.index]\n",
    "        brd_label = self.y[self.index]\n",
    "        reward = 1 if action == brd_label else 0  # Positive reward for correct action\n",
    "        self.index += 1\n",
    "        done = self.index >= self.num_samples  # Check if the episode is done\n",
    "        next_state = self.X[self.index] if not done else state  # Return the same state if done\n",
    "        return next_state, reward, done\n",
    "\n",
    "def train():\n",
    "    global epsilon\n",
    "    X_train, y_train, X_test, y_test = load_data()  # Load training and testing data\n",
    "\n",
    "    if X_train is None or y_train is None or X_test is None or y_test is None:\n",
    "        print(\"Error loading data.\")\n",
    "        return\n",
    "\n",
    "    # Initialize environment, model, and target model\n",
    "    env = CattleEnvironment(X_train, y_train)  # Pass training data to the environment\n",
    "    model = DQNModel()\n",
    "    target_model = DQNModel()\n",
    "\n",
    "    # Build the model by passing a dummy state\n",
    "    dummy_state = np.zeros((1, state_size))\n",
    "    model(dummy_state)\n",
    "    target_model(dummy_state)\n",
    "\n",
    "    # Copy weights from model to target_model\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    buffer = ReplayBuffer()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    episodes = 200  # Reduced the number of episodes\n",
    "    best_accuracy = 0.0  # Keep track of the best accuracy\n",
    "\n",
    "    # Store results in a list\n",
    "    results = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(model, state, epsilon)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Only store and train if the returned values are valid\n",
    "            if next_state is not None:\n",
    "                buffer.store(state, action, reward, next_state, done)\n",
    "                total_reward += reward\n",
    "                train_dqn(model, target_model, buffer, optimizer, X_train)  # Pass the entire training set\n",
    "                state = next_state\n",
    "\n",
    "        # Update epsilon\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        # Update target model weights every target_update_interval episodes\n",
    "        if episode % target_update_interval == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = calculate_accuracy(model, X_test, y_test)\n",
    "        results.append([episode, accuracy * 100, epsilon, total_reward])  # Append results\n",
    "\n",
    "        # Save the model if it's the best so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            print(f\"New best accuracy: {best_accuracy:.4f}, saving model...\")\n",
    "\n",
    "    # Convert results to DataFrame and print\n",
    "    results_df = pd.DataFrame(results, columns=['Episode', 'Accuracy (%)', 'Epsilon', 'Total Reward'])\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best Accuracy on Test Set: {best_accuracy * 100:.2f}%\")\n",
    "    print(results_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
